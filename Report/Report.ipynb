{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "environment = 'RocketLander-v0'     # Environment name\n",
    "\n",
    "# Agent\n",
    "gamma = 0.99                        # Reward discount factor\n",
    "learning_rate = 5e-5                # Learning rate\n",
    "num_episodes = 10                   # number of episodes\n",
    "max_steps_ep = 1000                 # default max number of steps per episode (unless env has a lower hardcoded limit)\n",
    "update_target = 100                 # number of steps to use slow target as target before updating it to latest weights\n",
    "epsilon_start = 1.0                 # probability of random action at start\n",
    "epsilon_end = 0.05                  # minimum probability of random action after linear decay period\n",
    "epsilon_decay_length = 1e5          # number of steps over which to linearly decay epsilon\n",
    "epsilon_decay_exp = 0.97            # exponential decay rate after reaching epsilon_end (per episode)\n",
    "\n",
    "# Brain\n",
    "huber_loss_delta = 1.0              # huber loss delta\n",
    "save_model_episode = 100            # interval to save model\n",
    "\n",
    "# Memory\n",
    "batch_size = 1024                   # size of batch from experience replay memory for updates\n",
    "memory_capacity = int(1e6)   # capacity of experience replay memory\n",
    "\n",
    "# Start environment\n",
    "env = gym.make(environment)\n",
    "\n",
    "# State and action variables\n",
    "stateCnt  = env.env.observation_space.shape[0]\n",
    "actionCnt = env.env.action_space.n\n",
    "\n",
    "# set seeds to 0\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# prepare monitorings\n",
    "monitorDir = 'videos'\n",
    "env = wrappers.Monitor(env, monitorDir, force=True, video_callable=lambda episode_id: episode_id%100==0)\n",
    "\n",
    "# prepare models\n",
    "modelDir = 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain\n",
    "\n",
    "The `Brain` class encapsulates the Neural Network. It was defined with 4 hidden layers with 512 neurons each and `ReLU` activation function. The input number of neurons is the number of states and the output number of neuros is the number of actions.\n",
    "\n",
    "This network is trained in order to aproximate the Q function and the target model is a simple copy of the model, however it is updated more sporadically.\n",
    "\n",
    "For the loss it is using the Huber loss function, it is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "# Huber loss function\n",
    "def huber_loss(y_true, y_pred):\n",
    "    \n",
    "    err = y_true - y_pred\n",
    "\n",
    "    cond = K.abs(err) < huber_loss_delta\n",
    "    L2 = 0.5 * K.square(err)\n",
    "    L1 = huber_loss_delta * (K.abs(err) - 0.5 * huber_loss_delta)\n",
    "\n",
    "    loss = tf.where(cond, L2, L1)\n",
    "\n",
    "    return K.mean(loss)\n",
    "\n",
    "class Brain:\n",
    "    \n",
    "    # Initialize brain\n",
    "    def __init__(self, stateCnt, actionCnt, batchSize):\n",
    "        self.stateCnt = stateCnt                 # number of states\n",
    "        self.actionCnt = actionCnt               # number os actions\n",
    "        self.batchSize = batchSize               # batch size\n",
    "\n",
    "        self.model = self.createModel()          # model\n",
    "        self.targetModel = self.createModel()    # target model\n",
    "\n",
    "    # Create model\n",
    "    def createModel(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=512, activation='relu', input_dim=stateCnt))\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "        model.add(Dense(units=512, activation='relu'))\n",
    "        model.add(Dense(units=actionCnt, activation='linear'))\n",
    "        model.compile(loss=huber_loss, optimizer=RMSprop(lr=learning_rate))\n",
    "        return model\n",
    "\n",
    "    # Train model using batch of random examples\n",
    "    def train(self, x, y, batchSize=batch_size, epochs=1, verbose=0):\n",
    "        self.model.fit(x, y, batch_size=batchSize, epochs=epochs, verbose=verbose)\n",
    "\n",
    "    # Predict using normal or target model given a batch of states\n",
    "    def predict(self, s, target=False):\n",
    "        if target:\n",
    "            return self.targetModel.predict(s)\n",
    "        else:\n",
    "            return self.model.predict(s)\n",
    "\n",
    "    # Predict given only one state\n",
    "    def predictOne(self, s, target=False):\n",
    "        return self.predict(s.reshape(1, self.stateCnt), target=target).flatten()\n",
    "\n",
    "    # Update target model \n",
    "    def updateTargetModel(self):\n",
    "        self.targetModel.set_weights(self.model.get_weights())\n",
    "        \n",
    "    # Save target model\n",
    "    def saveModel(self, episode):\n",
    "        self.targetModel.save(modelDir + \"/\" + environment + str(episode) + \".h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "The `Memory` class is necessary to store the experience that will be used for experience replay. A `deque` was chosen because it is a list-like container with fast appends and pops on either end.\n",
    "\n",
    "Each experience saved to the memory will have the following information:\n",
    "\n",
    "`(current_state, action, reward, next_state)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Memory:\n",
    "    \n",
    "    # Initialize memory\n",
    "    def __init__(self, capacity):\n",
    "        self.samples = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    # Add sample to the memory\n",
    "    def add(self, sample):\n",
    "        self.samples.append(sample)\n",
    "\n",
    "    # Generate 'n' random samples from the memory\n",
    "    def sample(self, n):\n",
    "        n = min(n, len(self.samples))\n",
    "        return random.sample(self.samples, n)\n",
    "    \n",
    "    # Number of current samples in memory\n",
    "    def numberSamples(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The `replay` function is responsible for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    steps = 0\n",
    "    epsilon = epsilon_start\n",
    "    epsilon_linear_step = (epsilon_start-epsilon_end)/epsilon_decay_length\n",
    "\n",
    "    # Initialize agent\n",
    "    def __init__(self, stateCnt, actionCnt, memoryCapacity, updateTarget, batchSize):\n",
    "        self.stateCnt = stateCnt\n",
    "        self.actionCnt = actionCnt\n",
    "        self.updateTarget = updateTarget\n",
    "        \n",
    "        self.brain = Brain(stateCnt, actionCnt, batchSize)     # initialize brain\n",
    "        self.memory = Memory(memoryCapacity)                   # initialize memory\n",
    "\n",
    "    # Act based ond epsilon\n",
    "    def act(self, s):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.actionCnt-1)\n",
    "        else:\n",
    "            return np.argmax(self.brain.predictOne(s))\n",
    "\n",
    "    # Save experience update target model if necessary\n",
    "    def observe(self, sample):\n",
    "        self.memory.add(sample)\n",
    "\n",
    "        if (self.steps % self.updateTarget == 0):\n",
    "            self.brain.updateTargetModel()\n",
    "            \n",
    "    # Decrement epsilon\n",
    "    def decrementEpsilon(self, done):\n",
    "        self.steps += 1\n",
    "        \n",
    "        # linearly decay epsilon from epsilon_start to epsilon_end over epsilon_decay_length steps\n",
    "        if self.steps < epsilon_decay_length:\n",
    "            self.epsilon -= self.epsilon_linear_step\n",
    "        # then exponentially decay it every episode\n",
    "        elif done:\n",
    "            self.epsilon *= epsilon_decay_exp\n",
    "        \n",
    "    # Replay saved data\n",
    "    def replay(self):\n",
    "        batch = self.memory.sample(batch_size)\n",
    "        batchLen = len(batch)\n",
    "\n",
    "        no_state = np.zeros(self.stateCnt)\n",
    "\n",
    "        states = np.array([ o[0] for o in batch ])\n",
    "        states_ = np.array([ (no_state if o[3] is None else o[3]) for o in batch ])\n",
    "\n",
    "        p = self.brain.predict(states)\n",
    "        p_ = self.brain.predict(states_, target=True)\n",
    "\n",
    "        x = np.zeros((batchLen, self.stateCnt))\n",
    "        y = np.zeros((batchLen, self.actionCnt))\n",
    "\n",
    "        for i in range(batchLen):\n",
    "            o = batch[i]\n",
    "            s = o[0]; a = o[1]; r = o[2]; s_ = o[3]\n",
    "\n",
    "            t = p[i]\n",
    "            if s_ is None:\n",
    "                t[a] = r\n",
    "            else:\n",
    "                t[a] = r + gamma * np.amax(p_[i])\n",
    "\n",
    "            x[i] = s\n",
    "            y[i] = t\n",
    "\n",
    "        self.brain.train(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "agent = Agent(stateCnt, actionCnt, memory_capacity, update_target, batch_size)\n",
    "\n",
    "# Populate memory\n",
    "while agent.memory.numberSamples() <= batch_size:\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # act\n",
    "        action = random.randint(0, actionCnt-1)\n",
    "\n",
    "        # execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # observe\n",
    "        agent.memory.add((state, action, reward, None if done else next_state))\n",
    "    \n",
    "for episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    steps_in_episode = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    for frame in range(max_steps_ep):\n",
    "        # act\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # execute action\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # update total reward\n",
    "        total_reward += reward\n",
    "\n",
    "        # observe\n",
    "        agent.observe((state, action, reward, None if done else next_state))\n",
    "\n",
    "        # decrement epsilon\n",
    "        agent.decrementEpsilon(done)\n",
    "        \n",
    "        # replay\n",
    "        agent.replay()\n",
    "\n",
    "        # update variables\n",
    "        state = next_state\n",
    "        steps_in_episode += 1\n",
    "\n",
    "        # save model\n",
    "        if (episode%save_model_episode==0):\n",
    "            agent.brain.saveModel(episode)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print('Episode %2i, Reward: %7.3f, Steps: %i, Next eps: %7.3f'%(episode,total_reward,steps_in_episode, agent.epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym]",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
